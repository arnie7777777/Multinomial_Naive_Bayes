{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWIJp4JTx1td"
      },
      "source": [
        "# Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pkH1OvWXx0og"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y) \n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5ohemuyLvm"
      },
      "source": [
        "In the multinomial Naive Bayes model, each document is represented as a bag of words and the number of occurrences of each word is used as a feature.\n",
        "\n",
        "Given a set of $m$ training documents, $C$ classes, and a vocabulary of $n$ words, let $x$ be a new document represented as a bag of words, where $x_i$ is the count of the i-th word in the vocabulary in the document. The goal is to find the class $y$ that maximizes the posterior probability, $P(y|x)$, using Bayes' Theorem:\n",
        "\n",
        ">$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
        "\n",
        "Here, $P(y)$ is the prior probability of the class, which can be estimated as the fraction of documents in the training set that belong to class y. $P(x|y)$ is the likelihood of the document given the class, which can be estimated as the product of the probabilities of each word in the vocabulary given the class. $P(x)$ is the normalizing constant, which is the same for all classes and can be ignored for the purposes of estimation.\n",
        "\n",
        "Using the multinomial distribution, the likelihood can be written as:\n",
        "\n",
        ">$P(x|y) = \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "Where $P(x_i|y)$ is the probability of observing word $i$ in a document given class $y$, which can be estimated as the count of word $i$ in class $y$ divided by the total count of all words in class $y$.\n",
        "\n",
        "Finally, taking the log of the posterior probabilities makes the calculation easier and allows us to find the MAP estimate by simply taking the maximum value:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log P(y|x) &= \\log \\frac{P(x|y)P(y)}{P(x)} \\\\\n",
        "&= \\log P(x|y) + \\log P(y) - \\log P(x) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "For prediction's we can ignore $\\log P(x)$ term and report the y that has highest $\\log P(y = i|x)$, where $i = {1,\\dots,c}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skTv0AJO5HwF"
      },
      "source": [
        "Above implementation has two missing parts. Let's develop it step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7DA8aFyopGhk"
      },
      "outputs": [],
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS2NM7s_5l19"
      },
      "source": [
        "**Q1: PART A [5 points]**\n",
        "\n",
        "Before writing any code, consider the following toy input. After fitting the dataset, what should be the value in `self.priors`?\n",
        "\n",
        "<!-- # we have two unique classes\n",
        "# we have 4 samples -->\n",
        "\n",
        "\n",
        "\n",
        "Note: you need to just write down the answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R668ansf6SOD"
      },
      "source": [
        "Solution:\n",
        "    Self.priors value should be \n",
        "The value for self.priors = 0.5, 0.5\n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PIthJ5MMx-4k"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG_d4FoO6UUb"
      },
      "source": [
        "\n",
        "**Q1: PART B [15 points]**\n",
        "\n",
        "In the `fit` function fill the missing line by uncommenting `self.priors[i] = ` and also remove `pass` after doing so. \n",
        "\n",
        "Note: Make edits in the below template.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g0z_u9VG6hoo"
      },
      "outputs": [],
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)     \n",
        "        for i in range(n_classes):             \n",
        "            self.priors[i] = np.count_nonzero([y == i]) / len(y)\n",
        "            \n",
        "        # Calculate the class-conditional feature probabilitie\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        log_probs = np.dot(X, self.counts.transpose())\n",
        "        # temp = []\n",
        "        # for i in range(len(X)) :\n",
        "        #     for j in range(len(X[0])) :\n",
        "                \n",
        "\n",
        "\n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FsdnL31MD7PW"
      },
      "outputs": [],
      "source": [
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NX4IzXoyjls"
      },
      "source": [
        "Here fit calculates the p(y) and also stores two sets of parameters p(x|y) for y=0 and y=1. Let's check these before proceeding. (run the below cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYakGrx7yiZ2",
        "outputId": "edb09006-2c92-4a91-f2a7-383bc2480711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5, 0.5])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb.priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgKXy9J8y5mJ",
        "outputId": "61886657-1dfa-4473-fa9b-d9503c01cbcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 4)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb.counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXt-esstzlBQ",
        "outputId": "1914efc1-01da-4eb8-f950-0594f667e17d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.11764706, 0.23529412, 0.23529412, 0.17647059],\n",
              "       [0.25      , 0.1875    , 0.125     , 0.1875    ]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb.counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I61FIcvuyjDi"
      },
      "source": [
        "**Q1: PART C [15 points]**\n",
        "\n",
        "Complete the `log_probs = ` line in predict function. (code block is pasted below)\n",
        "\n",
        "Note: If you can't implement in a single line you are free to write it in multiple lines. If your implementation is correct below `assert` statement should run with no error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IFo0S0YSpGhp"
      },
      "outputs": [],
      "source": [
        "# X @ self.counts.transpose\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_5hLOnmB0yDV"
      },
      "outputs": [],
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):         \n",
        "            self.priors[i] = np.count_nonzero([y == i]) / len(y)\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))  # sum over everything in array, then divide it by the total count \n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "      # returns a list of class labels for each x in X.\n",
        "      # Calculate the log probability of each class for each sample\n",
        "      log_probs = (X @ self.counts.transpose()) + np.log(self.priors) \n",
        "        # for ()\n",
        "\n",
        "      # self.counts = posteriors P(x given y)AKA counts, arrays of poesteriors for each document   \n",
        "      # Return the class with the highest log probability for each sample\n",
        "      return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "00SpKDb88Nei"
      },
      "outputs": [],
      "source": [
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "\n",
        "assert np.all(nb.predict(X)==y),\"your predictions are wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDz_PuKexylM"
      },
      "source": [
        "Now let's test the effectivness of this algorithm on a real-world data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QR48XYIoteNl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#look at the below cells and understand the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEiXOOnk1grq"
      },
      "source": [
        "This data set has 20 different types of news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKf5h-6n-Paj",
        "outputId": "56ced50c-53da-42ce-f3aa-6be162ebd228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH8qCeWV1pw3",
        "outputId": "d14bf12c-bdc4-466a-8636-14a03dd2e6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(list(newsgroups_train.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "rMZHDfH608dF",
        "outputId": "8585fc5f-ef31-49bc-810a-e0f3ceb4c829"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#let's see a sample\n",
        "newsgroups_train.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KkXB0j1KSD",
        "outputId": "b206f7b4-e987-4d33-a6f3-96926135a5e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 'rec.autos')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#The target attribute is the integer index of the category\n",
        "newsgroups_train.target[0],list(newsgroups_train.target_names)[newsgroups_train.target[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Kb4O9z9vGP"
      },
      "source": [
        "**Q2: PART A [15 POINTS]**\n",
        "\n",
        "Before proceeding, we need to convert the text into Bag of words format(as we saw in toy example). Sklearn has a built in function for it. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for it below and fill the missing lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aUAbRv3P-miY"
      },
      "outputs": [],
      "source": [
        "# Convert the text data into a bag-of-words representation\n",
        "\n",
        "vectorizer = CountVectorizer().fit(newsgroups_train.data, newsgroups_test.data)\n",
        "\n",
        "# here do do the fit\n",
        "\n",
        "# vectorizer .fit, (fit createas a matrix representation, then transform makes the data into a matrix transformation)\n",
        "\n",
        "# here we do the transform \n",
        "X_train = vectorizer.transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Note both train and test data are newsgroups_train.data,newsgroups_test.data respectively. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg6Xv3rFpGhu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU8MOp3cAW1a"
      },
      "source": [
        "**Q2: PART B [15 POINTS]**\n",
        "\n",
        "If your implementation of all the above parts is correct, you should be able to run the below cell. Fill in the below cell for `accuracy = `and report your result ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYKp9Oz2AnzT"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCRwcNKrpGhu"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiAMATGcuQOY",
        "outputId": "cf04c2dc-e427-4ba2-cb4c-bc81265d9e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 6.173659054699947%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fit the multinomial Naive Bayes model on the training data\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Predict the class labels of the testing samples\n",
        "\n",
        "y_pred = mnb.predict(X_test) \n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "#accuracy = (correct labels) / all labels, correct labels = predicted_labels == actual_labels, count\n",
        "\n",
        "accuracy = np.count_nonzero(newsgroups_test.target == y_pred) / len(y_pred)\n",
        "\n",
        "# ah = accuracy_score(y_pred, newsgroups_test.target)\n",
        "# print(ah)\n",
        "\n",
        "print(\"Accuracy:\", f'{accuracy*100}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVtPN5h72WR5"
      },
      "source": [
        "**Q2: PART C [5 Points]**\n",
        "\n",
        "In your code for calculating the counts(`self.counts`) \n",
        "\n",
        "```\n",
        "self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "```\n",
        "\n",
        "```\n",
        " self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        " ```\n",
        "\n",
        " Why do we add 1 in the numerator and include n_features ? Give a short explanation ?\n",
        "\n",
        " Hint: It's called Laplace smoothing. Figure out why it's being used here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv_Fgswz_3UC"
      },
      "source": [
        "Solution: Adding 1 in the numerator is an example of Laplace smoothing, which is a technique used to prevent zero probabilities in the estimation of the probability distribution. The addition of 1 in the numerator ensures that there are no zero probabilities, as it adds a count of 1 to each feature for each class. The inclusion of n_features in the denominator is also part of the Laplace smoothing technique, as it ensures that the sum of the probabilities in each class adds up to 1. This helps to ensure the accuracy of the model's predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADX8v7EpGhv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-m5tItVhZfU"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF02MG5hpGhv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuR9P1Wk6PgX"
      },
      "source": [
        "Consider this data set\n",
        "\n",
        "| Feature 1 | Feature 2 | Class |\n",
        "|-----------|-----------|-------|\n",
        "| 1         | 1         | 0     |\n",
        "| 2.2         | 1.6         | 0     |\n",
        "| 2.5         | 1.8         | 0     |\n",
        "| 2.8         | 1.5         | 0     |\n",
        "| 2.9         | 1.2         | 0     |\n",
        "| 3.0        | 3.0        | 1     |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHPvPOo5oO8"
      },
      "source": [
        "\n",
        "Consider the above data and train a logistic regression model on this data.\n",
        "\n",
        "Here are the learned weights: \n",
        "> $w_0$=  -4.08673095\n",
        "\n",
        "> $w_1$= 0.33123783\n",
        "\n",
        "> $w_2$= 0.89782125\n",
        "\n",
        ">Here $w_0$ corresponds to the intercept.\n",
        "\n",
        "</br>\n",
        "\n",
        "Here are the equations we discussed for class-conditional probabilities on a data-point $x_i = [x_{i1}, x_{i2}, ..., x_{ip}]$ :\n",
        "\n",
        ">$ P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{z}} $ \n",
        "\n",
        ">$ P(y=0|\\mathbf{x_i}) = \\frac{e^{z}}{1 + e^{z}} $ \n",
        "\n",
        ">Where $z = w_0 + w_1 x_{i1} + w_2 x_{i2} + ... + w_p x_{ip}$\n",
        "\n",
        ">Here $w_0$ is the intercept, $w_1, w_2, ..., w_p$ are the coefficients of the predictor variables $x_{i1}$, $x_{i2}$, ..., $x_{ip}$ for the $i$-th training sample, respectively.\n",
        "\n",
        "**Q3: PART A [10 points]**\n",
        "\n",
        "What is the accuracy over training data? (show the steps)\n",
        "\n",
        "To calculate the accuracy over the training data, we first need to calculate the probability of each class (0 and 1) given the features $\\mathbf{xi}$. From the equation given, we can see that the probability of class 0 is given by $P(y=0|\\mathbf{xi}) = \\frac{e^{z}}{1 + e^{z}}$ and the probability of class 1 is given by $P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{z}}$.\n",
        "\n",
        "We can then calculate the $z$ value for each data point using the equation $z = w0 + w1 x{i1} + w2 x{i2} + â€¦ + wp x{ip}$, with the given values of $w0$ and $w_1$.\n",
        "\n",
        "The accuracy over the training data can then be calculated by comparing the predicted class (the class with the highest probability) to the actual class for each data point and summing the number of correct predictions.\n",
        "\n",
        "Sample 1: $z = -4.08673095 + 0.33123783 \\times 1 + 0.89782125 \\times 1 = -2.863662$ \n",
        "- This uses weights 0, 1 & 3, as well as features 1 & 2\n",
        "\n",
        "$P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{-2.863662}} = 0.053$\n",
        "\n",
        "$P(y=0|\\mathbf{x_i}) = \\frac{e^{-2.863662}}{1 + e^{-2.863662}} = 0.947$\n",
        "\n",
        "Sample 2: $z = -4.08673095 + 0.33123783 \\times 2.2 + 0.89782125 \\times 1.6 = -1.835$\n",
        "\n",
        "$P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{-1.835}} = 0.164$\n",
        "\n",
        "$P(y=0|\\mathbf{x_i}) = \\frac{e^{-1.835}}{1 + e^{-1.835}} = 0.836$\n",
        "\n",
        "Sample 3: $z = -4.08673095 + 0.33123783 \\times 2.5 + 0.89782125 \\times 1.8 = -1.420$\n",
        "\n",
        "$P(y=1|\\mathbf{x_i}) = \\frac{1}{1 + e^{-1.420}} = 0.191$\n",
        "\n",
        "$P(y=0|\\mathbf{x_i}) = \\frac{e^{-1.420}}{1 + e^{-1.420}} = 0.809$\n",
        "\n",
        "Sample 4: $z = -4.08673095 + 0.33123783 \\times 2.8 + 0.89782125 \\times 1.5 = -1.065$\n",
        "................\n",
        "\n",
        "The accuracy over the training data is then calculated by summing the number of correct predictions (5 out of 6). 83.3% accuracy\n",
        "\n",
        "\n",
        "**Q3: PART B [10 points]**\n",
        "\n",
        "Is the decision boundary linear or non-linear (5 points)? \n",
        "\n",
        "-Linear\n",
        "\n",
        "Please write down the expression for the decision boundary (5 points).\n",
        "\n",
        "Equation for decision tree boundary:\n",
        "$$1 < \\frac{P(Y=0|X)}{P(Y=1|X)}$$\n",
        "$$ 1 < exp(w_{0}+\\sum_{i=1}^n w_{i}X_{i}) $$\n",
        "$$ 0 < (w_{0}+\\sum_{i=1}^n w_{i}X_{i}) $$\n",
        "$$ y = (w_{0}+\\sum_{i=1}^n w_{i}X_{i}) $$\n",
        "\n",
        "$$-4.08673095 + 0.33123783x{i1} + 0.89782125x_{i2} = y$$\n",
        "\n",
        "\n",
        "**Q3: PART C [10 points]**\n",
        "\n",
        "List all the classifiers we have learned, that are generative classifiers. What are their properties? (5 points)\n",
        "\n",
        "- Bayes(Naive)\n",
        "-Probability Tables\n",
        "- Generative classifiers have properties that incorporate probability. Mainly being able to learn from joint probability distributions, to which we then turn into our conditional probabilities and use to make predictions on new data points(which we use, such as in Naive Bayes).   \n",
        "\n",
        "List all the classifiers we have learned that are discriminative classifiers. What are their properties? (5 points)\n",
        "\n",
        "- Logisitic Regression, Deicsion Trees \n",
        "\n",
        "Discriminative classifiers properties work differently then generative in that they seek to find boundaries that seperate classes. This works by finding a decision boundary that BEST(with the least amount of error) separates the data into different classes. Logistic Regression uses linear boundaries such as we calculated up above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th4ecgitpGhw",
        "outputId": "4e03bbf5-7b32-4be9-e62d-7a66d7eb63cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 83.33%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# features and labels\n",
        "X = [[1, 1], [2.2, 1.6], [2.5, 1.8], [2.8, 1.5], [2.9, 1.2], [3.0, 3.0]]\n",
        "y = [0, 0, 0, 0, 0, 1]\n",
        "\n",
        "# create and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# make predictions\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# calculate the accuracy\n",
        "accuracy = accuracy_score(y, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_9mk_U_I8m"
      },
      "source": [
        "Solution:\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "0dbc88b9cfa46a4d23a017eb64c33f784f53a385dfb740de48073376a5d0662d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
